{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag name : h1, tag content : Main Page\n",
      "tag name : h2, tag content : From today's featured article\n",
      "tag name : h2, tag content : Did you know ...\n",
      "tag name : h2, tag content : In the news\n",
      "tag name : h2, tag content : On this day\n",
      "tag name : h2, tag content : Today's featured picture\n",
      "tag name : h2, tag content : Other areas of Wikipedia\n",
      "tag name : h2, tag content : Wikipedia's sister projects\n",
      "tag name : h2, tag content : Wikipedia languages\n",
      "tag name : h2, tag content : Navigation menu\n",
      "tag name : h3, tag content : Personal tools\n",
      "tag name : h3, tag content : Namespaces\n",
      "tag name : h3, tag content : Variants\n",
      "tag name : h3, tag content : Views\n",
      "tag name : h3, tag content : More\n",
      "tag name : h3, tag content : Search\n",
      "tag name : h3, tag content : Navigation\n",
      "tag name : h3, tag content : Contribute\n",
      "tag name : h3, tag content : Tools\n",
      "tag name : h3, tag content : Print/export\n",
      "tag name : h3, tag content : In other projects\n",
      "tag name : h3, tag content : Languages\n"
     ]
    }
   ],
   "source": [
    "def wikipedia(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    tags = ['h1','h2','h3','h4','h5','h6']\n",
    "    for tag in soup.find_all(tags):\n",
    "        print(\"tag name : \"+ tag.name + \", tag content : \" +tag.text.strip())\n",
    "\n",
    "wikipedia(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Movie Name  Year Rating\n",
      "0  The Shawshank Redemption  1994    9.2\n",
      "1             The Godfather  1972    9.1\n",
      "2    The Godfather: Part II  1974    9.0\n",
      "3           The Dark Knight  2008    9.0\n",
      "4              12 Angry Men  1957    8.9\n"
     ]
    }
   ],
   "source": [
    "def movie(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    tmovie = soup.find_all('td',class_='titleColumn')\n",
    "    rmovie = soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "    movie_name =[]\n",
    "    movie_rating=[]\n",
    "    movie_year=[]\n",
    "    \n",
    "    for i in range(0,100):\n",
    "        movie_name.append(tmovie[i].a.text)\n",
    "        movie_rating.append(rmovie[i].strong.text)\n",
    "        year = tmovie[i].span.text.strip('()')\n",
    "        movie_year.append(year)\n",
    "    movies = pd.DataFrame({'Movie Name':movie_name,\n",
    "                       'Year':movie_year,\n",
    "                       'Rating':movie_rating})   \n",
    "    print(movies.head())\n",
    "    \n",
    "movie(\"https://www.imdb.com/chart/top/?ref_=nv_mv_250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Movie Name  Year Rating\n",
      "0    Pather Panchali  1955    8.5\n",
      "1            Nayakan  1987    8.5\n",
      "2         Anbe Sivam  2003    8.5\n",
      "3  Pariyerum Perumal  2018    8.5\n",
      "4            Golmaal  1979    8.5\n"
     ]
    }
   ],
   "source": [
    "def movie(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    tmovie = soup.find_all('td',class_='titleColumn')\n",
    "    rmovie = soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "    movie_name =[]\n",
    "    movie_rating=[]\n",
    "    movie_year=[]\n",
    "    \n",
    "    for i in range(0,100):\n",
    "        movie_name.append(tmovie[i].a.text)\n",
    "        movie_rating.append(rmovie[i].strong.text)\n",
    "        year = tmovie[i].span.text.strip('()')\n",
    "        movie_year.append(year)\n",
    "    movies = pd.DataFrame({'Movie Name':movie_name,\n",
    "                       'Year':movie_year,\n",
    "                       'Rating':movie_rating})   \n",
    "    print(movies.head())\n",
    "    \n",
    "movie(\"https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=2e9dfa9b-3e4d-4d39-acd2-8af11f252a59&pf_rd_r=FW65WMWNQP9XWSDFKNAK&pf_rd_s=right-5&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_india_tr_rhs_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Book Name                                Genre  \\\n",
      "0              ★ Revival Season                Fiction/Coming of Age   \n",
      "1  ★ The Cot in the Living Room   Children's/Children's Picture Book   \n",
      "2                  King Richard  Nonfiction/History/American History   \n",
      "3                        Heaven                Fiction/Coming of Age   \n",
      "4                    The Guncle                 Fiction/Family Drama   \n",
      "\n",
      "                                   Author  \\\n",
      "0                             Monica West   \n",
      "1  Hilda Eunice Burgos, Gaby D'Alessandro   \n",
      "2                           Michael Dobbs   \n",
      "3    Mieko Kawakami, Sam Bett, David Boyd   \n",
      "4                           Steven Rowley   \n",
      "\n",
      "                                              Review  \n",
      "0  The events of Monica West’s debut novel, Reviv...  \n",
      "1  Author Hilda Eunice Burgos’ heartfelt first pi...  \n",
      "2  Though it’s been eclipsed in the minds of many...  \n",
      "3  If you are an avid reader, you might have been...  \n",
      "4  At the beginning of Steven Rowley’s third nove...  \n"
     ]
    }
   ],
   "source": [
    "def Scrap_book(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    Books=[]\n",
    "    Genre=[]\n",
    "    Author=[]\n",
    "    Review=[]\n",
    "    rev=\"\"\n",
    "    books = soup.find_all('h4',class_='italic')\n",
    "    genre = soup.find_all('p',class_='genre-links hidden-phone')\n",
    "    author = soup.find_all('p',class_='sans bold')\n",
    "    review = soup.find_all('div',class_='read-full')\n",
    "    for i in range(0,5):\n",
    "        Books.append(books[i].a.text.strip())\n",
    "        Author.append(author[i].text.strip())\n",
    "        Gen=[]\n",
    "        for gen in genre[i].find_all('a'):\n",
    "            Gen.append(gen.text)\n",
    "        Genre.append(\"/\".join(Gen))\n",
    "        for link in review[i].find_all('a'):\n",
    "            url=\"https://bookpage.com/\"+link['href']\n",
    "            subpage = requests.get(url)\n",
    "            subSoup = BeautifulSoup(subpage.content)\n",
    "            content = subSoup.find('div',class_='article-body')\n",
    "            content = content.find_all('p')\n",
    "            rev= content[0].text+content[1].text+content[2].text+content[3].text + content[4].text \n",
    "        Review.append(rev.strip())\n",
    "    movies = pd.DataFrame({'Book Name':Books,\n",
    "                       'Genre':Genre,\n",
    "                       'Author':Author,\n",
    "                       'Review':Review})   \n",
    "    print(movies.head())\n",
    "\n",
    "Scrap_book(\"https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches Points Rating\n",
      "0   New Zealand      17  2,054    121\n",
      "1     Australia      25  2,945    118\n",
      "2         India      29  3,344    115\n",
      "3       England      27  3,100    115\n",
      "4  South Africa      20  2,137    107\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tr')\n",
    "    team=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    rating=[]\n",
    "    top = trows[1].find('td',class_='rankings-block__banner--team-name')\n",
    "    team.append(top.find('span',class_='u-hide-phablet').text)\n",
    "    matches.append(trows[1].find('td',class_='rankings-block__banner--matches').text)\n",
    "    points.append(trows[1].find('td',class_='rankings-block__banner--points').text)\n",
    "    rating.append(trows[1].find('td',class_='rankings-block__banner--rating').text.strip())\n",
    "    for i in range(2,11):\n",
    "        top = trows[i].find('td',class_='table-body__cell rankings-table__team')\n",
    "        team.append(top.find('span',class_='u-hide-phablet').text)\n",
    "        matches.append(trows[i].find_all('td',class_='table-body__cell u-center-text')[0].text)\n",
    "        points.append(trows[i].find_all('td',class_='table-body__cell u-center-text')[1].text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    Cricket = pd.DataFrame({'Team':team,\n",
    "                       'Matches':matches,\n",
    "                       'Points':points,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket.head())\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Player Team Rating\n",
      "0           Babar Azam  PAK    865\n",
      "1          Virat Kohli  IND    857\n",
      "2         Rohit Sharma  IND    825\n",
      "3          Ross Taylor   NZ    801\n",
      "4          Aaron Finch  AUS    791\n",
      "5       Jonny Bairstow  ENG    785\n",
      "6         Fakhar Zaman  PAK    778\n",
      "7  Francois du Plessis   SA    778\n",
      "8         David Warner  AUS    773\n",
      "9            Shai Hope   WI    773\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tbody')\n",
    "    trows = trows[0].find_all('tr')\n",
    "    player=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    topName = div[0].text\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    topteam = div[0].text.strip()\n",
    "    topteam=topteam[:3].strip()\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    toprating = div[0].text\n",
    "    player.append(topName)\n",
    "    team.append(topteam)\n",
    "    rating.append(toprating)\n",
    "    for i in range(0,9):\n",
    "        player.append(trows[i].find('td',class_='table-body__cell name').a.text)\n",
    "        team.append(trows[i].find('td',class_='table-body__cell nationality-logo').find('span',class_='table-body__logo-text').text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "        Cricket = pd.DataFrame({'Player':player,\n",
    "                       'Team':team,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket)\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0        Trent Boult   NZ    737\n",
      "1       Mehedi Hasan  BAN    725\n",
      "2   Mujeeb Ur Rahman  AFG    708\n",
      "3         Matt Henry   NZ    691\n",
      "4     Jasprit Bumrah  IND    690\n",
      "5      Kagiso Rabada   SA    666\n",
      "6       Chris Woakes  ENG    665\n",
      "7     Josh Hazlewood  AUS    660\n",
      "8  Mustafizur Rahman  BAN    652\n",
      "9        Pat Cummins  AUS    646\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tbody')\n",
    "    trows = trows[1].find_all('tr')\n",
    "    player=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    topName = div[1].text\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    topteam = div[1].text.strip()\n",
    "    topteam=topteam[:3].strip()\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    toprating = div[1].text\n",
    "    player.append(topName)\n",
    "    team.append(topteam)\n",
    "    rating.append(toprating)\n",
    "    for i in range(0,9):\n",
    "        player.append(trows[i].find('td',class_='table-body__cell name').a.text)\n",
    "        team.append(trows[i].find('td',class_='table-body__cell nationality-logo').find('span',class_='table-body__logo-text').text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "        Cricket = pd.DataFrame({'Player':player,\n",
    "                       'Team':team,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket)\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches Points Rating\n",
      "0     Australia      18  2,955    164\n",
      "1  South Africa      24  2,828    118\n",
      "2       England      17  1,993    117\n",
      "3         India      20  2,226    111\n",
      "4   New Zealand      21  1,947     93\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tr')\n",
    "    team=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    rating=[]\n",
    "    top = trows[1].find('td',class_='rankings-block__banner--team-name')\n",
    "    team.append(top.find('span',class_='u-hide-phablet').text)\n",
    "    matches.append(trows[1].find('td',class_='rankings-block__banner--matches').text)\n",
    "    points.append(trows[1].find('td',class_='rankings-block__banner--points').text)\n",
    "    rating.append(trows[1].find('td',class_='rankings-block__banner--rating').text.strip())\n",
    "    for i in range(2,11):\n",
    "        top = trows[i].find('td',class_='table-body__cell rankings-table__team')\n",
    "        team.append(top.find('span',class_='u-hide-phablet').text)\n",
    "        matches.append(trows[i].find_all('td',class_='table-body__cell u-center-text')[0].text)\n",
    "        points.append(trows[i].find_all('td',class_='table-body__cell u-center-text')[1].text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    Cricket = pd.DataFrame({'Team':team,\n",
    "                       'Matches':matches,\n",
    "                       'Points':points,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket.head())\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0     Tammy Beaumont  ENG    765\n",
      "1        Lizelle Lee   SA    758\n",
      "2       Alyssa Healy  AUS    756\n",
      "3    Stafanie Taylor   WI    746\n",
      "4        Meg Lanning  AUS    723\n",
      "5  Amy Satterthwaite   NZ    715\n",
      "6    Smriti Mandhana  IND    710\n",
      "7        Mithali Raj  IND    709\n",
      "8     Natalie Sciver  ENG    685\n",
      "9    Laura Wolvaardt   SA    683\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tbody')\n",
    "    trows = trows[0].find_all('tr')\n",
    "    player=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    topName = div[0].text\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    topteam = div[0].text.strip()\n",
    "    topteam=topteam[:3].strip()\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    toprating = div[0].text\n",
    "    player.append(topName)\n",
    "    team.append(topteam)\n",
    "    rating.append(toprating)\n",
    "    for i in range(0,9):\n",
    "        player.append(trows[i].find('td',class_='table-body__cell name').a.text)\n",
    "        team.append(trows[i].find('td',class_='table-body__cell nationality-logo').find('span',class_='table-body__logo-text').text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "        Cricket = pd.DataFrame({'Player':player,\n",
    "                       'Team':team,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket)\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player Team Rating\n",
      "0    Marizanne Kapp   SA    418\n",
      "1      Ellyse Perry  AUS    418\n",
      "2   Stafanie Taylor   WI    410\n",
      "3    Natalie Sciver  ENG    349\n",
      "4     Deepti Sharma  IND    343\n",
      "5     Jess Jonassen  AUS    307\n",
      "6  Ashleigh Gardner  AUS    252\n",
      "7  Dane van Niekerk   SA    243\n",
      "8     Sophie Devine   NZ    242\n",
      "9       Amelia Kerr   NZ    236\n"
     ]
    }
   ],
   "source": [
    "def cricket_ranking(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    trows = soup.find_all('tbody')\n",
    "    trows = trows[2].find_all('tr')\n",
    "    player=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    topName = div[2].text\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    topteam = div[2].text.strip()\n",
    "    topteam=topteam[:3].strip()\n",
    "    div = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    toprating = div[2].text\n",
    "    player.append(topName)\n",
    "    team.append(topteam)\n",
    "    rating.append(toprating)\n",
    "    for i in range(0,9):\n",
    "        player.append(trows[i].find('td',class_='table-body__cell name').a.text)\n",
    "        team.append(trows[i].find('td',class_='table-body__cell nationality-logo').find('span',class_='table-body__logo-text').text)\n",
    "        rating.append(trows[i].find('td',class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "        Cricket = pd.DataFrame({'Player':player,\n",
    "                       'Team':team,\n",
    "                       'Rating':rating})   \n",
    "    print(Cricket)\n",
    "cricket_ranking(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Name   Price  \\\n",
      "0    Redmi 9 (Sky Blue, 4GB RAM, 64GB Storage)| 500...   8,799   \n",
      "1    Redmi 9A (Nature Green, 2GB RAM, 32GB Storage)...   6,999   \n",
      "2    Samsung Galaxy M31 (Ocean Blue, 6GB RAM, 128GB...  14,999   \n",
      "3    Samsung Galaxy M12 (Blue,4GB RAM, 64GB Storage...  10,999   \n",
      "4    Oppo A31 (Mystery Black, 6GB RAM, 128GB Storag...  10,990   \n",
      "..                                                 ...     ...   \n",
      "415  Samsung Galaxy M12 (Blue,6GB RAM, 128GB Storag...  12,499   \n",
      "416  Samsung Galaxy M11 (Metallic Blue, 4GB RAM, 64...   8,999   \n",
      "417      KECHAODA K115 with Dual Sim 1.44\" inch (Gold)   1,099   \n",
      "418  Samsung Galaxy M31 (Ocean Blue, 6GB RAM, 128GB...  14,999   \n",
      "419  Tecno Spark 7 (Spruce Green, 2GB RAM, 32 GB St...   7,499   \n",
      "\n",
      "                                                 Image              Rating  \n",
      "0    https://m.media-amazon.com/images/I/71A9Vo1Bat...  4.2 out of 5 stars  \n",
      "1    https://m.media-amazon.com/images/I/71sxlhYhKW...  4.2 out of 5 stars  \n",
      "2    https://m.media-amazon.com/images/I/71-Su4Wr0H...  4.3 out of 5 stars  \n",
      "3    https://m.media-amazon.com/images/I/71yYaNztZ0...  4.0 out of 5 stars  \n",
      "4    https://m.media-amazon.com/images/I/71KCwNV6Mu...  4.2 out of 5 stars  \n",
      "..                                                 ...                 ...  \n",
      "415  https://m.media-amazon.com/images/I/71yYaNztZ0...  4.0 out of 5 stars  \n",
      "416  https://m.media-amazon.com/images/I/71GQUxuSpn...  4.2 out of 5 stars  \n",
      "417  https://m.media-amazon.com/images/I/71dFFje233...  3.9 out of 5 stars  \n",
      "418  https://m.media-amazon.com/images/I/71-Su4Wr0H...  4.3 out of 5 stars  \n",
      "419  https://m.media-amazon.com/images/I/719b6ihbwZ...  3.9 out of 5 stars  \n",
      "\n",
      "[420 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def mobiles(url):\n",
    "    flag=True\n",
    "    count=0\n",
    "    name =[]\n",
    "    price =[]\n",
    "    image =[]\n",
    "    rating =[]\n",
    "    while(flag):\n",
    "        page = requests.get(url,headers=HEADERS)\n",
    "        soup = BeautifulSoup(page.content,'html.parser')\n",
    "        page = soup.find_all('div',class_=\"sg-col-inner\")\n",
    "        page = page[2].find_all('div',class_=\"s-include-content-margin s-border-bottom s-latency-cf-section\")\n",
    "        for i in range(0,len(page)):\n",
    "            na = page[i].find('span',class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "            pr = page[i].find('span',class_=\"a-price-whole\")\n",
    "            im = page[i].find('img',class_=\"s-image\")\n",
    "            ra = page[i].find('span',class_=\"a-icon-alt\")\n",
    "            if(na!=None):\n",
    "                name.append(na.text)\n",
    "            else:\n",
    "                name.append(\"NaN\")\n",
    "            if(pr!=None):\n",
    "                price.append(pr.text)\n",
    "            else:\n",
    "                price.append(\"NaN\")\n",
    "            if(len(im)==0):\n",
    "                image.append(im['src'])\n",
    "            else:\n",
    "                image.append(\"NaN\")\n",
    "            if(ra!=None):\n",
    "                rating.append(ra.text)\n",
    "            else:\n",
    "                rating.append(\"NaN\")        \n",
    "        nextpage=soup.find('li',class_='a-last')\n",
    "        anch = nextpage.find('a')\n",
    "        if(count>18):\n",
    "            flag=False\n",
    "        else:\n",
    "            z = anch['href']\n",
    "            url=\"https://www.amazon.in\" + z\n",
    "            \n",
    "        count+=1\n",
    "    Mobiles = pd.DataFrame({'Name':name,\n",
    "                       'Price':price,\n",
    "                       'Image':image,\n",
    "                      'Rating':rating})\n",
    "    print(Mobiles)\n",
    "mobiles('https://www.amazon.in/s?k=mobiles&rh=p_36%3A-2000000&page=1&qid=1622099921&rnid=1318502031&ref=sr_pg_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Period                                Short_desc  Temperature  \\\n",
      "0          Today                 Sunny thenSunny andBreezy  High: 65 °F   \n",
      "1        Tonight   Mostly Clearand Breezythen PartlyCloudy   Low: 51 °F   \n",
      "2         Friday                              Mostly Sunny  High: 64 °F   \n",
      "3    FridayNight  Partly Cloudyand Breezythen MostlyCloudy   Low: 51 °F   \n",
      "4       Saturday          Partly Sunnythen Sunnyand Breezy  High: 65 °F   \n",
      "5  SaturdayNight    Mostly Clearand Breezythen MostlyClear   Low: 51 °F   \n",
      "6         Sunday                                     Sunny  High: 70 °F   \n",
      "7    SundayNight                              Mostly Clear   Low: 53 °F   \n",
      "8    MemorialDay                                     Sunny  High: 77 °F   \n",
      "\n",
      "                                         Description  \n",
      "0  Today: Sunny, with a high near 65. Breezy, wit...  \n",
      "1  Tonight: Mostly clear, with a low around 51. B...  \n",
      "2  Friday: Mostly sunny, with a high near 64. Wes...  \n",
      "3  Friday Night: Partly cloudy, with a low around...  \n",
      "4  Saturday: Mostly sunny, with a high near 65. B...  \n",
      "5  Saturday Night: Mostly clear, with a low aroun...  \n",
      "6                Sunday: Sunny, with a high near 70.  \n",
      "7  Sunday Night: Mostly clear, with a low around 53.  \n",
      "8          Memorial Day: Sunny, with a high near 77.  \n"
     ]
    }
   ],
   "source": [
    "def Weather(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    data = soup.find(id=\"seven-day-forecast\")\n",
    "    period_tags = data.select(\".tombstone-container .period-name\")\n",
    "    period = [pt.get_text() for pt in period_tags]\n",
    "    shortd = [sd.get_text() for sd in data.select(\".tombstone-container .short-desc\")]\n",
    "    temperature = [t.get_text() for t in data.select(\".tombstone-container .temp\")]\n",
    "    description = [d[\"title\"] for d in data.select(\".tombstone-container img\")]\n",
    "    weather = pd.DataFrame({\n",
    "            \"Period\": period,\n",
    "             \"Short_desc\": shortd,\n",
    "             \"Temperature\": temperature,\n",
    "             \"Description\":description\n",
    "        })\n",
    "    print(weather)\n",
    "Weather(\"https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YK-jlKgzZPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape fresher job listings from ‘https://internshala.com/’. It should include job title, company name, CTC, and apply date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Job_Title  \\\n",
      "0                               Business Analyst   \n",
      "1  Business Development Executive (Inside Sales)   \n",
      "2                 Business Development Associate   \n",
      "3                   Full Stack Software Engineer   \n",
      "4                  Associate Front End Developer   \n",
      "\n",
      "                                name         CTC    Apply date  \n",
      "0  Kasper Consulting Private Limited  26 Jun' 21  3.25 - 4 LPA  \n",
      "1                            GREedge  26 Jun' 21      3.75 LPA  \n",
      "2                   PEPKIDZ LEARNING  25 Jun' 21     3 - 4 LPA  \n",
      "3                         Flair Labs  25 Jun' 21     3 - 4 LPA  \n",
      "4                  Little Big Things  25 Jun' 21     3 - 5 LPA  \n"
     ]
    }
   ],
   "source": [
    "def gather_information(url):\n",
    "    job=[]\n",
    "    name=[]\n",
    "    loc=[]\n",
    "    date=[]\n",
    "    sal = []\n",
    "    for i in range(2,4):\n",
    "        res = requests.get(url)\n",
    "        markup = res.content\n",
    "        soup = BeautifulSoup(markup,'lxml')\n",
    "        posts = soup.find_all('div',class_='container-fluid individual_internship visibilityTrackerItem')\n",
    "        for r in posts:\n",
    "            job.append(r.find('div',class_='heading_4_5').text.strip())\n",
    "            name.append(r.find('a',class_='link_display_like_text').text.strip())\n",
    "            date.append(r.find_all('div',class_='item_body',id=False)[0].text.strip())\n",
    "            sal.append(r.find_all('div',class_='item_body',id=False)[1].text.strip())\n",
    "        url = \"https://internshala.com/fresher-jobs/page-\" + str(i)\n",
    "    jobs = pd.DataFrame({\n",
    "            \"Job_Title\": job,\n",
    "             \"name\": name,\n",
    "             \"CTC\": sal,\n",
    "             \"Apply date\":date\n",
    "        })\n",
    "    print(jobs.head())\n",
    "gather_information(\"https://internshala.com/fresher-jobs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
